import os
import io
import streamlit as st
from PIL import Image
from dotenv import load_dotenv
from huggingface_hub import InferenceClient

load_dotenv()

HF_API_KEY = os.getenv("HUGGINGFACE_API_KEY")
# Using the newer FLUX model which is optimized for the new router
MODEL_ID = "black-forest-labs/FLUX.1-schnell"

def generate_image_hf(prompt):
    """Generate an image using Hugging Face InferenceClient."""
    if not HF_API_KEY:
        return None, "Hugging Face API Key is missing. Please add HUGGINGFACE_API_KEY to your .env file."

    try:
        # InferenceClient automatically handles the correct endpoint (api-inference or router)
        client = InferenceClient(token=HF_API_KEY)
        
        # Standard text-to-image call
        image = client.text_to_image(
            prompt,
            model=MODEL_ID
        )
        
        if image:
            return image, None
        else:
            return None, "No image was generated by the model."
            
    except Exception as e:
        error_msg = str(e)
        if "503" in error_msg:
            return None, "The model is currently loading or busy. Please try again in a few seconds."
        elif "401" in error_msg:
            return None, "Invalid Hugging Face API Key. Please check your .env file."
        return None, f"Hugging Face Hub Error: {error_msg}"
